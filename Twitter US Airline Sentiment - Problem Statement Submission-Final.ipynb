{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc122f54",
   "metadata": {
    "id": "bc122f54"
   },
   "source": [
    "## Etan Ogbemi\n",
    "### Background and Context:\n",
    "\n",
    "Twitter possesses 330 million monthly active users, which allows businesses to reach a broad population and connect with customers without intermediaries. On the other hand, there’s so much information that it’s difficult for brands to quickly detect negative social mentions that could harm their business.\n",
    "\n",
    "That's why sentiment analysis/classification, which involves monitoring emotions in conversations on social media platforms, has become a key strategy in social media marketing.\n",
    "\n",
    "\n",
    "Listening to how customers feel about the product/service on Twitter allows companies to understand their audience, keep on top of what’s being said about their brand and their competitors, and discover new trends in the industry.\n",
    "\n",
    " \n",
    "\n",
    "Data Description:\n",
    "\n",
    "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "\n",
    " \n",
    "\n",
    "Dataset:\n",
    "\n",
    "The dataset has the following columns:\n",
    "\n",
    "* tweet_id\n",
    "* airline_sentiment\n",
    "* airline_sentiment_confidence\n",
    "* negativereason\n",
    "* negativereason_confidence\n",
    "* airline\n",
    "* airline_sentiment_gold\n",
    "* name\n",
    "* negativereason_gold\n",
    "* retweet_count\n",
    "* text\n",
    "* tweet_coord\n",
    "* tweet_created\n",
    "* tweet_location\n",
    "* user_timezone\n",
    "\n",
    "#### Objective\n",
    "This project will attempt to analyse the sentiments of airline passengers in order to glean information about what passengers feel about various airlines they travel on.  It will also try to make some determinations as to why the passangers have those sentiments.\n",
    "\n",
    "We will build and tune a predictive model that we hope will accurately predict the features that cause the sentiments which passengers express using \"unseen\"/test data, which in turn will provide insights for airlines to improve their services and generate more positive sentiments from their passengers by improving the travel experience.  All of this will be achieved by using the techniques of Sentiment analysis, Encoding techniques, data analysis, feature selection, data pre processing and vectorization amongst other techniques we have learnt in this module and throughout this programme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "F-ePeGPEpV_d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1607,
     "status": "ok",
     "timestamp": 1650686731704,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "F-ePeGPEpV_d",
    "outputId": "dfa4754e-b0f1-4a4f-e013-6b439c740854"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b982bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11846,
     "status": "ok",
     "timestamp": 1650687568498,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "96b982bb",
    "outputId": "f53762a4-7575-478e-d3c2-164277482c08"
   },
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import numpy as np                                  \n",
    "import pandas as pd                              \n",
    "import nltk                                     \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Downloading NLTL lexicons for use by VADER\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "!pip install vaderSentiment\n",
    "!pip install textblob\n",
    "!pip install contractions\n",
    "!pip install wordcloud\n",
    "\n",
    "from nltk.corpus import stopwords                   #Stopwords corpus\n",
    "# from nltk.corpus import vader_lexicon               #vader_lexicon corpus\n",
    "from nltk.stem import PorterStemmer                 # Stemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator, wordcloud\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier       # Import Random forest Classifier\n",
    "from sklearn.metrics import classification_report         # Import Classification report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold      \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import contractions\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer\n",
    "# set threshold for compound score, we will vary this \n",
    "# def classify_compound(text, threshold=0.33):\n",
    "\n",
    "from textblob import TextBlob\n",
    "# In order to be able to view long text strings we set the option for maximum width\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    " \n",
    "# for plottting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c65230",
   "metadata": {
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1650686758372,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "33c65230"
   },
   "outputs": [],
   "source": [
    "# Import the datat set\n",
    "data_original = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/NLP/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8bf7c",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1650686758373,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "23f8bf7c"
   },
   "outputs": [],
   "source": [
    "# copying data to another varaible to avoid any changes to original data\n",
    "data_copy = data_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52CYNbUuu6Vg",
   "metadata": {
    "id": "52CYNbUuu6Vg"
   },
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "We will look at the dataset to determine information about the data and understand it's structure and other properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ee615",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1650686758373,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "778ee615",
    "outputId": "e80f2991-2f95-4ce4-8708-18e9f7d98ae6"
   },
   "outputs": [],
   "source": [
    "data_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d00dd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 982
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1650686758374,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "d6d00dd1",
    "outputId": "44680baa-8bfc-4df3-f2d4-c0cf7965cd70"
   },
   "outputs": [],
   "source": [
    "# viewing the first few rows of the data\n",
    "data_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0f78d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650686758374,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "efc0f78d",
    "outputId": "e9cf40cc-9ee4-46ee-8066-462e818009ef"
   },
   "outputs": [],
   "source": [
    "data_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eBhaxi1Gg4JO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1650686758375,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "eBhaxi1Gg4JO",
    "outputId": "f6c7bb0f-78c1-450c-8829-9e609317ec5f"
   },
   "outputs": [],
   "source": [
    "# distribution of tweets accross airlines\n",
    "print()\n",
    "sns.countplot(data_copy['airline']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gnzIO-Sy1noz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650686758375,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "gnzIO-Sy1noz",
    "outputId": "ac86a431-c18a-4c9b-8846-145d090ee9ef"
   },
   "outputs": [],
   "source": [
    "# Distribution of sentiments accross all tweets\n",
    "sns.countplot(data_copy['airline_sentiment']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R3Mjb3bYeebn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1650686758713,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "R3Mjb3bYeebn",
    "outputId": "19ed3d49-72fe-4541-f88f-7ae1ad88457f"
   },
   "outputs": [],
   "source": [
    "# Distribution of sentiments by airline\n",
    "print()\n",
    "sns.countplot(data_copy['airline'], hue=data_copy['airline_sentiment']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KQGprxku5ie8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1650686759413,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "KQGprxku5ie8",
    "outputId": "97e6e5b0-a59d-4a88-b784-9b8f60970600"
   },
   "outputs": [],
   "source": [
    "# Distribution of all negative reasons\n",
    "plt.figure(figsize=(15,10)) #adjust the size of plot\n",
    "ax=sns.countplot(x=data_copy['negativereason'],data=data_copy,palette='magma')\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")  #it will rotate text on x axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6nXvvTUeKx17",
   "metadata": {
    "id": "6nXvvTUeKx17"
   },
   "source": [
    "### Observation\n",
    "In 2015 when this data was scraped, the [biggest airlines](https://en.wikipedia.org/wiki/List_of_largest_airlines_in_North_America) (from wikipedia and other sources) in terms of passengers enplaned (from online research) in descending order of the airplanes in this survey was American Airlines, Delta Air lines, Southwest Airlines, United Airlines, US Airway and Virgin America.  It however useful to note that United received more tweets than (probably mostly negative from our sentiment analysis below) than bigger airlines like American, Delta and South West! \n",
    "\n",
    "From our analysis, we can see that there were more than 4 times as many negative tweets as there were positive tweets, there were also thrice as many negative tweets than there were tweets classified as neutral.  From this, we could be inclined to surmise that people are more likely to tweet when they have a negative sentiment than when they have a positive or neutral sentiment about their travel experience.  However when we view Virgin America and Delta, it appears that the unbalance between negative and positive sentiments is less pronounced.  This may suggest that when travellers receive exceptionally good service, they might be just as inclined to tweet about their experience as when the have good experiences.\n",
    "\n",
    "One may also draw the conclusion that there is an inverse proportional relationship between the size of an airline and customer satisfaction.  This could be for a number of reasons like smaller airlines work harder to increase market share, larger airlines do not have as much incentive to increase their the quality of their service as they are already large, maintaining quality service becomes more challenging with size or a combination of  these factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb68e3f",
   "metadata": {
    "id": "fcb68e3f"
   },
   "source": [
    "### Observations\n",
    "\n",
    "It may be useful to note that the dataset comprises of not just the scraped data from the tweets of users, but also contains sentiment analysis that has already been carried out, some of which may be inaccurate.\n",
    "\n",
    "For example when viewed in an excel spreadsheet, for row 84 with tweet_id 569933405506310000, the \"airline_sentiment\" is classified as \"negative\" with a confidence of almost 70%.  Further, the reason for the negative classification is given in the feature \"negativereason\" as \"Late Flight\".  However if we examine the actual tweet by the customer, in the feature \"text\" the tweet is **\"@VirginAmerica you're the best!! Whenever I (begrudgingly) use any other airline I'm delayed and Late Flight\".**  This actually appears to be more of a positive sentiment about Virgin America than a negative sentiment even though the words \"Late Flight' appear in the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132620bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1650686759414,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "132620bc",
    "outputId": "62d12cd1-f79c-41dd-b19f-d1fffd7299d1"
   },
   "outputs": [],
   "source": [
    "data_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6Kkl7Kw2xYMV",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650686759414,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "6Kkl7Kw2xYMV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "POKyIGMoqumk",
   "metadata": {
    "id": "POKyIGMoqumk"
   },
   "source": [
    "### Data pre processing\n",
    "\n",
    "There are several features that we will drop which have no utility for our sentiment analysis, these include tweet_id, negativereason_confidence, airline_sentiment_gold, name, negativereason_gold, retweet_count, tweet_coord, tweet_created, tweet_location, user_timezone.  Apart from the fact that many contain several thousand NaN values, their meaning is unclear.  Since we have a unique index, others like tweet_id and name are redundant.\n",
    "\n",
    "We will also clean up the text data to remove nonalphabetical characters like @ signs, numbers and replace contractions.  We will also Lemmatize and Tokenize the text data as well as remove stopwords.  From reviewing the data however, we notice that the text (that is the tweets) always begins with the twitter handle of the airline eg @united, @VirginAmerica etc, so rather than remove just the @ sign, we will remove the whole twitter handle for the 6 airlines from the text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdefff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1650686759414,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "cfcdefff",
    "outputId": "a58aeb77-d9f4-4e26-caa5-d724444f66d1"
   },
   "outputs": [],
   "source": [
    "data_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1caaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 982
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1650686759415,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "74e1caaf",
    "outputId": "60687fbe-c880-4b86-e34b-b11d8a26c07a"
   },
   "outputs": [],
   "source": [
    "data_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bi1n7JXRAiHN",
   "metadata": {
    "id": "bi1n7JXRAiHN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oG1u7L4kMHOs",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1650686759415,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "oG1u7L4kMHOs"
   },
   "outputs": [],
   "source": [
    "#  We will remove the tweet handles (which start with @) and links (which start with http) for the airlines using the re module for Regular Expressions, I have chosen to do this at this point to make the wordcloud neater\n",
    "# More data preparation will be done later\n",
    "import re\n",
    "\n",
    "def remove_twitter_handles(text):\n",
    "    text = re.sub('@[^\\s]+','',text)\n",
    "    text = re.sub('http[^\\s]+','',text)\n",
    "    return text\n",
    "data_copy['text'] = data_copy['text'].apply(remove_twitter_handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frgJwJEMMHmV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1650686759415,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "frgJwJEMMHmV",
    "outputId": "646344a7-1e05-4008-d2d6-606d6eb1e3f4"
   },
   "outputs": [],
   "source": [
    "data_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3R0EBCRHi9Ry",
   "metadata": {
    "id": "3R0EBCRHi9Ry"
   },
   "source": [
    "### Changing the name of the text column from \"text\" to \"passenger_tweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dxl96g9MHnM",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650686759415,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "7dxl96g9MHnM"
   },
   "outputs": [],
   "source": [
    "data_copy.rename(columns = {'text':'passenger_tweet'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iqgbuE6zMH-x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 982
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650686759416,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "iqgbuE6zMH-x",
    "outputId": "9ab721f7-b9e9-41c9-c0c5-8ee0c9aadb8f"
   },
   "outputs": [],
   "source": [
    "data_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rhjYn-jiBjJt",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650686759416,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "rhjYn-jiBjJt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2yQM3GG0rguC",
   "metadata": {
    "id": "2yQM3GG0rguC"
   },
   "source": [
    "### Wordcloud for positive and negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cQc7TZEBGE",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650686759417,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "72cQc7TZEBGE"
   },
   "outputs": [],
   "source": [
    "# data['airline_sentiment'] = data_copy.apply(lambda row: nltk.word_tokenize(row['airline_sentiment']), axis=1) # Tokenization of data\n",
    "\n",
    "def show_wordcloud(data_copy, title):\n",
    "    text = ' '.join(data_copy['passenger_tweet'].astype(str).tolist())                 # Converting Summary column into list\n",
    "    stopwords = set(wordcloud.STOPWORDS)                                  # instantiate the stopwords from wordcloud\n",
    "    \n",
    "    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='white', max_words=75,         # Setting the different parameter of stopwords and limiting to 75 words\n",
    "                    colormap='viridis', width=800, height=600).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(14,11), frameon=True)                             \n",
    "    plt.imshow(fig_wordcloud)  \n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2CK9zcx0BjTZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 3761,
     "status": "ok",
     "timestamp": 1650686763166,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "2CK9zcx0BjTZ",
    "outputId": "fdac74a9-5784-408a-9f7a-7a723d963700"
   },
   "outputs": [],
   "source": [
    "# Negative sentiment wordcloud\n",
    "show_wordcloud(data_copy[data_copy.airline_sentiment == \"negative\"], title = \"Negative sentiment wordcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5yARg5PuBjde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "executionInfo": {
     "elapsed": 2003,
     "status": "ok",
     "timestamp": 1650686765166,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "5yARg5PuBjde",
    "outputId": "31407fea-d499-4bdd-dd31-fa4954d89bdd"
   },
   "outputs": [],
   "source": [
    "# Positive sentiment wordcloud\n",
    "show_wordcloud(data_copy[data_copy.airline_sentiment == \"positive\"], title = \"Positive_sentiment wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GeiI6mRsBgUR",
   "metadata": {
    "id": "GeiI6mRsBgUR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Ss0AbX5T-tWU",
   "metadata": {
    "id": "Ss0AbX5T-tWU"
   },
   "source": [
    "Since our objective, as I indicated earlier, is to perform sentiment analysis based on tweets from customers, our focus will be solely on two of the columns(features) in the dataset, \"airline_sentiment\" which is a subjective classification of the sentiments expressed in the customer tweets and \"text\", the actual tweets from the customers in which we hope some sentiment is expressed.\n",
    "\n",
    "We will therefore drop all the other columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_NtsZ-E2yTet",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650686765167,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "_NtsZ-E2yTet"
   },
   "outputs": [],
   "source": [
    "# dropping all columns except for airline_sentiment and text\n",
    "data_copy.drop([\"tweet_id\", \"airline_sentiment_confidence\", \"negativereason\", \"negativereason_confidence\", \"airline\", \"airline_sentiment_gold\", \"name\", \"negativereason_gold\", \"retweet_count\", \"tweet_coord\", \"tweet_created\", \"tweet_location\", \"user_timezone\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HqmVV78Tz5IT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650686765167,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "HqmVV78Tz5IT",
    "outputId": "ea683ee8-cd10-4087-b012-f85d6b94e265"
   },
   "outputs": [],
   "source": [
    "# Checking to ensure only two features are remaining\n",
    "data_copy.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yNB7xbIPEaEG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650686765168,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "yNB7xbIPEaEG",
    "outputId": "8bbca849-64d0-4658-e9ae-ff66ee68096a"
   },
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "data_copy['passenger_tweet'] = data_copy['passenger_tweet'].apply(lambda x: replace_contractions(x))\n",
    "# data['Summary'] = data['Summary'].apply(lambda x: replace_contractions(x))\n",
    "\n",
    "data_copy.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FiX10YMzFToS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1650686765465,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "FiX10YMzFToS",
    "outputId": "279acc2a-8272-4573-b9a7-801d61fdd51b"
   },
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "  return text\n",
    "\n",
    "data_copy['passenger_tweet'] = data_copy['passenger_tweet'].apply(lambda x: remove_numbers(x))\n",
    "\n",
    "data_copy.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8TA3tkCPFxzW",
   "metadata": {
    "executionInfo": {
     "elapsed": 3949,
     "status": "ok",
     "timestamp": 1650686769412,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "8TA3tkCPFxzW"
   },
   "outputs": [],
   "source": [
    "data_copy['passenger_tweet'] = data_copy.apply(lambda row: nltk.word_tokenize(row['passenger_tweet']), axis=1) # Tokenization of data\n",
    "# data['Summary'] = data.apply(lambda row: nltk.word_tokenize(row['Summary']), axis=1) # Tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O8SSX5t8HGZ6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1650686769413,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "O8SSX5t8HGZ6",
    "outputId": "c5644fd6-aa5b-4a70-d3d0-88e510df4f5a"
   },
   "outputs": [],
   "source": [
    "# Confirm tokenization\n",
    "data_copy.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hWEj5Yen0o_q",
   "metadata": {
    "id": "hWEj5Yen0o_q"
   },
   "source": [
    "### Data pre-processing\n",
    "\n",
    "We will process the data further by converting all the words in the tweets to lower case, replace contractions, remove any numbers, remove any other stopwords, tokenize the tweets and also lemmatize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egmPjHml87qI",
   "metadata": {
    "executionInfo": {
     "elapsed": 3939,
     "status": "ok",
     "timestamp": 1650686773342,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "egmPjHml87qI"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_list(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "      new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "#    words = remove_stopwords(words)\n",
    "    words = lemmatize_list(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "data_copy['passenger_tweet'] = data_copy.apply(lambda row: normalize(row['passenger_tweet']), axis=1)\n",
    "# data['Summary'] = data.apply(lambda row: normalize(row['Summary']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sCRznZ0gquSM",
   "metadata": {
    "executionInfo": {
     "elapsed": 672,
     "status": "ok",
     "timestamp": 1650688106510,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "sCRznZ0gquSM"
   },
   "outputs": [],
   "source": [
    "# Vectorization (Convert text data to numbers).\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "Count_vec = CountVectorizer(max_features=2500)                # Keep only 500 features as number of features will increase the processing time.\n",
    "data_features = Count_vec.fit_transform(data_copy['passenger_tweet'])\n",
    "\n",
    "data_features = data_features.toarray()                        # Convert the data features to array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zOBAJxNWNR2R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1650686773344,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "zOBAJxNWNR2R",
    "outputId": "e68bd7c6-44c8-4286-e832-b12ec3034329"
   },
   "outputs": [],
   "source": [
    "data_copy.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-g78tvJ5IAmv",
   "metadata": {
    "id": "-g78tvJ5IAmv"
   },
   "source": [
    "### Observation\n",
    "\n",
    "I note that some of the lemmatizations and contractions have were not perfect, for example in row 19 the word bosfil was contracted from BOS-FLL and in row 15 SFO-PDX becaomes sfopdx.  These will tend to be unique words and therefore should have very limited impact during learning and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DTeDmf5shMJM",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650686773344,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "DTeDmf5shMJM"
   },
   "outputs": [],
   "source": [
    "data_copy = data_copy.replace(['positive'],'1')\n",
    "data_copy = data_copy.replace(['neutral'],'1')\n",
    "data_copy = data_copy.replace(['negative'],'0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vU6_QdOiiNtm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650686773344,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "vU6_QdOiiNtm",
    "outputId": "46323ed7-25b7-4272-aa0a-1d5651442837"
   },
   "outputs": [],
   "source": [
    "data_copy.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ILALfJvwXnM",
   "metadata": {
    "id": "5ILALfJvwXnM"
   },
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y7N_mOxFJcqh",
   "metadata": {
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1650686774486,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "Y7N_mOxFJcqh"
   },
   "outputs": [],
   "source": [
    "# Vectorization (Convert text data to numbers).\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vec = CountVectorizer(max_features=2500)                # We will use only 2,500 features and discard the rest\n",
    "data_features = bow_vec.fit_transform(data_copy['passenger_tweet'])\n",
    "\n",
    "data_features = data_features.toarray()                        # Convert the data features to array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wdGB-prDJQMO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650686774486,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "wdGB-prDJQMO",
    "outputId": "e040d2ff-3859-491f-bacd-c9679af35c17"
   },
   "outputs": [],
   "source": [
    "data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TAitsN4pfedI",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650686774487,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "TAitsN4pfedI"
   },
   "outputs": [],
   "source": [
    "X = data_features\n",
    "\n",
    "y = data_copy.airline_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4DOk1notkhAt",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650686774487,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "4DOk1notkhAt"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jrw9CPSQkoFF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650686774487,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "Jrw9CPSQkoFF",
    "outputId": "568f4b10-1e11-4e6d-bb43-021db4335661"
   },
   "outputs": [],
   "source": [
    "# Finding optimal number of base learners using k-fold CV ->\n",
    "base_ln = np.arange(100,400,100).tolist()\n",
    "base_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tiW2iYc0koOL",
   "metadata": {
    "executionInfo": {
     "elapsed": 354526,
     "status": "ok",
     "timestamp": 1650687129009,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "tiW2iYc0koOL"
   },
   "outputs": [],
   "source": [
    "# K-Fold Cross - validation .\n",
    "cv_scores = []\n",
    "for b in base_ln:\n",
    "    clf = RandomForestClassifier(n_estimators = b)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv = 5, scoring = 'accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7GeA5p0EkoUL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650687129010,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "7GeA5p0EkoUL",
    "outputId": "5d182d1a-eadc-4339-e0c1-d46ad4c2e8b1"
   },
   "outputs": [],
   "source": [
    "# plotting the error as k increases\n",
    "error = [1 - x for x in cv_scores]                                 #error corresponds to each nu of estimator\n",
    "optimal_learners = base_ln[error.index(min(error))]                #Selection of optimal nu of n_estimator corresponds to minimum error.\n",
    "plt.plot(base_ln, error)                                           #Plot between each nu of estimator and misclassification error\n",
    "xy = (optimal_learners, min(error))\n",
    "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
    "plt.xlabel(\"Number of base learners\")\n",
    "plt.ylabel(\"Misclassification Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mZdIP2QQocXS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20120,
     "status": "ok",
     "timestamp": 1650687599906,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "mZdIP2QQocXS",
    "outputId": "ab5b6c31-3a16-4705-eae4-1484515ce365"
   },
   "outputs": [],
   "source": [
    "# Training the best model and calculating accuracy on test data .\n",
    "clf = RandomForestClassifier(n_estimators = optimal_learners)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "count_vectorizer_predicted = clf.predict(X_test)\n",
    "print(classification_report(y_test ,count_vectorizer_predicted , target_names = ['0' , '1']))\n",
    "print(\"Accuracy of the model is : \",accuracy_score(y_test,count_vectorizer_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P4wNzi1SkoZq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1650687605942,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "P4wNzi1SkoZq",
    "outputId": "1211d3e8-f77f-495e-9478-49c1afeef6b0"
   },
   "outputs": [],
   "source": [
    "# Confusion matirx to get an idea of how the distribution of the prediction is, among all the classes.\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, count_vectorizer_predicted)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['0', '1']],\n",
    "                  columns = [i for i in ['0', '1']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W1rrMN7zzXPZ",
   "metadata": {
    "id": "W1rrMN7zzXPZ"
   },
   "source": [
    "While overall the true positives are high and the talse negative and false positive's are comparitively lower, the model can be expected to perform better.  Nonetheless an accuracy of 84% is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ga80ciLSkod1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "executionInfo": {
     "elapsed": 3650,
     "status": "ok",
     "timestamp": 1650688395378,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "Ga80ciLSkod1",
    "outputId": "e00b9acb-8648-45fb-d388-86d01f56acfe"
   },
   "outputs": [],
   "source": [
    "all_features = Count_vec.get_feature_names()              #Instantiate the feature from the vectorizer\n",
    "top_features=''                                            # Addition of top 40 feature into top_feature after training the model\n",
    "feat=clf.feature_importances_\n",
    "features=np.argsort(feat)[::-1]\n",
    "for i in features[0:50]:\n",
    "    top_features+=all_features[i]\n",
    "    top_features+=','\n",
    "    \n",
    "print(top_features)  \n",
    "\n",
    "print(\" \") \n",
    "print(\" \")     \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color=\"white\",colormap='tab20c',width=2000, \n",
    "                          height=1000).generate(top_features)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.figure(1, figsize=(20, 18), frameon='true')\n",
    "plt.title('Top 50 features WordCloud', fontsize=20)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zhOCRrMGswss",
   "metadata": {
    "id": "zhOCRrMGswss"
   },
   "source": [
    "### Term Frequency - Inverse Document Frequency\n",
    "\n",
    "TF–IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. [Wikipedia](https://https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nB0x80ipndL-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1650688728522,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "nB0x80ipndL-",
    "outputId": "5470ca21-7908-4d98-d8ba-85c2833a13a0"
   },
   "outputs": [],
   "source": [
    "# Using TfidfVectorizer to convert text data to numbers.\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_features=500)\n",
    "data_features = tfidf_vect.fit_transform(data_copy['passenger_tweet'])\n",
    "\n",
    "data_features = data_features.toarray()\n",
    "\n",
    "data_features.shape     #feature shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YfkzosEbndQZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1650688833707,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "YfkzosEbndQZ"
   },
   "outputs": [],
   "source": [
    "X = data_features\n",
    "\n",
    "y = data_copy.airline_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TGVKVwVgndVQ",
   "metadata": {
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1650688844619,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "TGVKVwVgndVQ"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing set.\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XT3GKupot7Qb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1650688846585,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "XT3GKupot7Qb",
    "outputId": "0d102bce-3ad9-4e5a-f13f-f77222da52a9"
   },
   "outputs": [],
   "source": [
    "# Finding optimal number of base learners using k-fold CV ->\n",
    "base_ln = np.arange(100,400,100).tolist()\n",
    "base_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tvsKig85t7bf",
   "metadata": {
    "executionInfo": {
     "elapsed": 124990,
     "status": "ok",
     "timestamp": 1650688992006,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "tvsKig85t7bf"
   },
   "outputs": [],
   "source": [
    "# K-Fold Cross - validation .\n",
    "cv_scores = []\n",
    "for b in base_ln:\n",
    "    clf = RandomForestClassifier(n_estimators = b)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv = 5, scoring = 'accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z9uO3jidt7ga",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650688992007,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "Z9uO3jidt7ga",
    "outputId": "63b0dd21-4930-43f2-fcb9-05595d5b83b0"
   },
   "outputs": [],
   "source": [
    "# plotting the error as k increases\n",
    "error = [1 - x for x in cv_scores]                                 #error corresponds to each nu of estimator\n",
    "optimal_learners = base_ln[error.index(min(error))]                #Selection of optimal nu of n_estimator corresponds to minimum error.\n",
    "plt.plot(base_ln, error)                                           #Plot between each nu of estimator and misclassification error\n",
    "xy = (optimal_learners, min(error))\n",
    "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
    "plt.xlabel(\"Number of base learners\")\n",
    "plt.ylabel(\"Misclassification Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W2H3VtaHt7kz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4365,
     "status": "ok",
     "timestamp": 1650688996364,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "W2H3VtaHt7kz",
    "outputId": "60ee9b30-2441-455b-d8d0-d14d57d372d1"
   },
   "outputs": [],
   "source": [
    "# Training the best model and calculating accuracy on test data .\n",
    "clf = RandomForestClassifier(n_estimators = optimal_learners)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "tf_idf_predicted = clf.predict(X_test)\n",
    "print(classification_report(y_test , tf_idf_predicted , target_names = ['0' , '1']))\n",
    "print(\"Accuracy of the model is : \",accuracy_score(y_test,tf_idf_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HcNadGXKueoG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1650688996651,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "HcNadGXKueoG",
    "outputId": "98048d7b-1915-4563-af45-3ab6eb2d024d"
   },
   "outputs": [],
   "source": [
    "# Print and plot Confusion matirx to get an idea of how the distribution of the prediction is, among all the classes.\n",
    "\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, tf_idf_predicted)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['0', '1']],\n",
    "                  columns = [i for i in ['0', '1']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "McORJs6Mz1ML",
   "metadata": {
    "id": "McORJs6Mz1ML"
   },
   "source": [
    "The performance of the model is pretty decent with true positives being dominant and true negatives less so.  they both are more than the false negatives and positives, in this case false negatives being a bit closer to true negatives.  This is reflected in the slightly better performance of the BoW whn compared to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oJyX-JMtues5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "executionInfo": {
     "elapsed": 3834,
     "status": "ok",
     "timestamp": 1650689166098,
     "user": {
      "displayName": "Etan O",
      "userId": "07025373812308569463"
     },
     "user_tz": 300
    },
    "id": "oJyX-JMtues5",
    "outputId": "270da856-430e-409f-bc27-9989d8b17782"
   },
   "outputs": [],
   "source": [
    "all_features = tfidf_vect.get_feature_names()              #Instantiate the feature from the vectorizer\n",
    "top_features=''                                            # Addition of top 40 feature into top_feature after training the model\n",
    "feat=clf.feature_importances_\n",
    "features=np.argsort(feat)[::-1]\n",
    "for i in features[0:40]:\n",
    "    top_features+=all_features[i]\n",
    "    top_features+=', '\n",
    "    \n",
    "print(top_features)  \n",
    "\n",
    "print(\" \") \n",
    "print(\" \") \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color=\"white\",colormap='tab20c',width=2000, \n",
    "                          height=1000).generate(top_features)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.figure(1, figsize=(14, 11), frameon='equal')\n",
    "plt.title('Top 40 features WordCloud', fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3WaY1_QUxmXO",
   "metadata": {
    "id": "3WaY1_QUxmXO"
   },
   "source": [
    "## Performance comparison\n",
    "\n",
    "TF-IDF had an accuracy of 83% as compared to Bag of Words which had an accuracy of 84%.  Although the numbers are very close, the results were smewhat surprising as one would have expected TF-IDF to perform better due to it's weighting, which usually produces better accuracy than the counting of words used by BoW"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter US Airline Sentiment - Problem Statement Submission-Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
